### Задание
### 1. Intro: Airflow, DAG, scheduler

#### Challenge

1. Арендовать сервер, разрешить вход по ssh ключу для оргов (ключ будет в чате).
1. Развернуть на сервере Airflow, спрятанный за basic auth.
1. Написать и выкатить DAG, который ходит в
  [апишку Яндекса](https://yastat.net/s3/milab/2020/covid19-stat/data/data_struct_10.json?v=timestamp),
  тянет оттуда данные по коронавирусу и кладет их в .csv файлик, который можно скачать.

Требования к DAG:

- Запускается каждый день в 12:00 по Москве.
- Тянет все данные до текущего момента.
- Складывает данные по России.
- Колонки в csv: date, region, infected, recovered, dead.
----

### Результат
1. Готово. Сервер в Oracle Cloud.
1. Готово.
    - Интерфейс Airflow доступен по http://<server_ip>
    - Файл с результатом - по ссылке http://<server_ip>/stat/
    - Airflow metadata DB перенесена в поднятый на той же машине Postgresql.

1. Готово с ограничением. Полную структуру API не осилил. Для экономии времени ограничил исходные данные одним json
файлом по ссылке в задании.
    - DAG запускается в 9 по UTC (12 по Москве).
    - Тянет все доступные в файле даты (31 день).
    - Формирует CSV файл с задаными колонками.
---
#### todo
- запускать webserver и scheduler как сервисы, а то последний, похоже плохо работает в бекграунде.
Падает и DAGи не отрабатывают.