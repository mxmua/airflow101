### 2. Источники, сенсоры и хуки

#### Challenge

1. Сделать DAG из нескольких шагов, собирающий данные по заказам, проверяющий
   статусы транзакций через API и складывающий результат в базу данных
1. Данные по заказам брать в виде csv-файла
   [отсюда](https://airflow101.python-jitsu.club/orders.csv)
1. Данные по статусам транзакций хранятся в виде json
   [тут](https://api.jsonbin.io/b/5ed7391379382f568bd22822)
1. Данные о товарах и пользователях живут в PostgreSQL БД (доступ в тг чате).
1. Результаты складывать в другую PostgreSQL БД (доступ к ней будет у вас в
   личке). Именно в ней должен лежить финальный датасет.

Требования к DAG:

- На каждый источник должен быть один оператор.
- Повторные выполнения DAG должны обновлять имеющиеся данные. То есть,
  если статус транзакции изменился на "оплачен", то это должно быть
  отражено в датасете.
- Из любого источника могут приходить грязные данные. Делайте чистку:
  удаляйте дубли, стрипайте строки, проверяйте на null/None.
- Логин/пароль для доступа в postgres базу надо получить у оргов
  (dbname совпадает с вашим логином).
- Прежде чем писать в постгрес, надо убедиться, что там есть схема
  и таблица.
- Вы можете использовать pandas, но вообще в благородных домах
  pandas не тянут "в продакшен".
- Финальный датасет содержит следующие колонки: `name`, `age`,
  `good_title`, `date`, `payment_status`, `total_price`, `amount`, `last_modified_at`.
--- 

#### Решение

**DAG:** hw02

**Табл. результата:** final_dataset

**Плюсы:** работает.

**Минусы:** говнокод.


1. Попутно с ДЗ-02 переустановил Airflow и настроил запуск под systemd.
Теперь не падает scheduler, как в приличных домах. 
1. Многие шаги можно объединить или пропустить.
В образовательных целях наклепал их целую пачку, а переделать не успею. Зато набил руку.
Все-таки, сделал в SQL. Пока не хватает Python-овского воображения. 

1. Особенности чистки данных:
    - Раз возраст берется из базы клиентов, то и имя берем оттуда же, для консистентности.
    Считаем, что раз клиент в базе, значит раньше у него уже был успешный заказ на эти реквизиты.
    - Если бы в датасете были контактные данные, можно было бы предположить, что информация
    нужна для колл-центра. Тогда имя можно было бы брать из информации о заказе,
    как более свежего источника.
    - Промежуточные STAGE_* таблицы хранятся до момента следующего запуска DAGa,
    В реальном мире это нужно для разборок с по поводу качества данных.
    - Orders: в случае дублей по UUID - берем более свежий по дате.
    - Customers: в случае дублей по email - берется "случайная" запись. Нет подходящего критерия.
    - Goods: в случае дублей по имени - берется "случайная" запись.
    - Payment status: если среди дублей по uuid заказа есть успешный статус - берем его.

---
#### todo
- разобраться с ветвлением выполнения tasks